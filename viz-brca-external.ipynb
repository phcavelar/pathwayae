{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAVING_FORMATS = [\"pdf\", \"svg\", \"png\"]\n",
    "internal_valid_results_folder = \"results/\"\n",
    "results_folder = \"results/metabric\"\n",
    "images_folder = \"images/metabric\"\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "for fmt in SAVING_FORMATS: os.makedirs(os.path.join(images_folder,fmt), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVERTED_METRICS = [\"MSE\"]\n",
    "\n",
    "results_to_gather = [\n",
    "    'clf_meta_results_202303311447.csv',\n",
    "    'clf_meta_results_202304020106.csv',\n",
    "    'clf_meta_results_202304020436.csv',\n",
    "    'clf_meta_results_202304022211.csv',\n",
    "    'clf_meta_results_202304061522.csv',\n",
    "    'clf_meta_results_202304070049.csv'\n",
    "]\n",
    "\n",
    "test_results = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(os.path.join(results_folder,f), index_col=0) for f in results_to_gather\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").reset_index()\n",
    "\n",
    "for m in INVERTED_METRICS:\n",
    "    test_results[f\"Train {m}\"] = -test_results[f\"Train {m}\"]\n",
    "    test_results[f\"Test {m}\"] = -test_results[f\"Test {m}\"]\n",
    "\n",
    "test_results = test_results.rename(columns={\"AUC\":\"ROC AUC\"})\n",
    "\n",
    "test_results[\"Classifier\"] = test_results[\"Classifier\"].str.replace(\"SVC\", \"SVM\").str.replace(\"LogisticRegression\", \"LR\").str.replace(\"RandomForestClassifier\", \"RF\")\n",
    "test_results[\"Space Type\"] = test_results[\"Space Type\"].str.replace(\"Latent\", \"$z$\").str.replace(\"Pathway Activity\", \"$a$\")\n",
    "test_results.loc[np.logical_and(test_results[\"Space Type\"]==\"$z$\", test_results[\"Model\"].str.contains(\"VAE\")), \"Space Type\"] = \"$μ$\"\n",
    "\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_model_and_feat(k):\n",
    "    if isinstance(k,(pd.Index,)):\n",
    "        idx:pd.Index = k\n",
    "        return [sep_model_and_feat(i) for i in idx.values]\n",
    "    feature = k.split(\"-\")[0] + \" \" + \" \".join(k.split(\"-\")[1].split(\" \")[1:])\n",
    "    model = k.split(\"-\")[1].split(\" \")[0]\n",
    "    return model, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_latex(series:pd.Series) -> pd.Series:\n",
    "    return series.str.replace(\n",
    "        \"β\",\"$\\\\beta$\", regex=False\n",
    "    ).str.replace(\n",
    "        \"μ\",\"$\\\\mu$\", regex=False\n",
    "    ).str.replace(\n",
    "        \"-step-\",\"-$\\\\mathbbm{1}$-\", regex=False\n",
    "    ).str.replace(\n",
    "        \"-smooth-\",\"-$\\\\sigma$-\", regex=False\n",
    "    ).str.replace(\n",
    "        \"(Hallmark Genes)\",\"(HG)\", regex=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_cols = [\"Model Size\", \"Test MSE\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\"]\n",
    "table_results = test_results.copy()\n",
    "table_results[\"Model\"] = preprocess_for_latex(table_results[\"Model\"])\n",
    "table_results[\"Model Class\"] = preprocess_for_latex(table_results[\"Model Class\"])\n",
    "\n",
    "table_results.groupby(\n",
    "        [\"Classifier\",\"Model\",\"Space Type\",]\n",
    "    )[table_cols].agg(\n",
    "        np.median\n",
    "    )\n",
    "\n",
    "print(\n",
    "    table_results.groupby(\n",
    "        [\"Classifier\",\"Model\",\"Space Type\",]\n",
    "    )[table_cols].agg(\n",
    "        lambda x: f\"{np.quantile(x, q=0.50):.3f} ({np.quantile(x, q=0.75)-np.quantile(x, q=0.25):.3f})\"\n",
    "    ).sort_index(inplace=False).style.to_latex(\n",
    "        hrules=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See viz-brca.ipynb\n",
    "with open(os.path.join(internal_valid_results_folder,\"best.json\"), \"r\") as best_f:\n",
    "    best = json.load(best_f)\n",
    "    for k in best:\n",
    "        model_and_clf_name, best_score = best[k]\n",
    "        model_and_clf_name = preprocess_for_latex(pd.Series([model_and_clf_name]))[0]\n",
    "        best[k] = [\n",
    "            \"-\".join([model_and_clf_name.split(\"-\")[0], *model_and_clf_name.split(\"-\")[2:]]),\n",
    "            model_and_clf_name.split(\"-\")[1],\n",
    "            best_score,\n",
    "        ]\n",
    "\n",
    "{\n",
    "    'AE ': ('AE-[128, 64]', \"LR\", 0.7416842803471456),\n",
    "    'VAE ': ('VAE-$\\mathbbm{1}$-$\\\\beta$1-[128, 64]', \"LR\", 0.5126946725701951),\n",
    "    'PAAE (KEGG)': ('PAAE-[32]-[64] (KEGG)', \"LR\", 0.9743640361931267),\n",
    "    'PAAE (HG)': ('PAAE-[32]-[64] (HG)', \"LR\", 0.9695108184255021),\n",
    "    'PAVAE (KEGG)': ('PAVAE-$\\sigma$-$\\\\beta$1-[]-[128, 64] (KEGG)', \"SVM\", 0.8640078506643825),\n",
    "    'PAVAE (HG)': ('PAVAE-$\\mathbbm{1}$-$\\\\beta$1-[]-[128, 64] (HG)', \"SVM\", 0.9260142637119084),\n",
    "}\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdl_cls, (mdl, clf, auc) in best.items():\n",
    "    print(\n",
    "        mdl_cls,\n",
    "        mdl,\n",
    "        clf,\n",
    "        f\"{auc:.3f}\",\n",
    "    )\n",
    "    print(\n",
    "        table_results.loc[\n",
    "            np.logical_and(\n",
    "                table_results[\"Model\"] == mdl,\n",
    "                table_results[\"Classifier\"] == clf,\n",
    "            ),\n",
    "            [\"Model\", \"Classifier\", \"Space Type\"] + [\"ROC AUC\"]\n",
    "        ].groupby(\n",
    "            [\"Model\", \"Classifier\", \"Space Type\"]\n",
    "        ).agg(\n",
    "            lambda x: f\"{np.quantile(x, q=0.50):.3f} ({np.quantile(x, q=0.75)-np.quantile(x, q=0.25):.3f})\"\n",
    "        ).reset_index()\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_rank = \"ROC AUC\"\n",
    "bigger_is_better = True\n",
    "argbest_fn = (lambda series: series.argmax()) if bigger_is_better else (lambda series: series.argmin())\n",
    "table_cols = [\"Model Size\", \"Test MSE\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\"]\n",
    "table_results = test_results.copy()\n",
    "table_results[\"Model\"] = preprocess_for_latex(table_results[\"Model\"])\n",
    "table_results[\"Model Class\"] = preprocess_for_latex(table_results[\"Model Class\"])\n",
    "\n",
    "median_aggregates = table_results.groupby(\n",
    "        [\"Classifier\",\"Model\",\"Space Type\",]\n",
    "    )[table_cols].agg(\n",
    "        np.median\n",
    "    ).reset_index()\n",
    "\n",
    "median_aggregates[\"Model Class\"] = median_aggregates[\"Model\"].map({k:v for (k,v) in table_results[[\"Model\",\"Model Class\"]].to_numpy()})\n",
    "\n",
    "best_of_each_class = []\n",
    "for m in median_aggregates[\"Model Class\"].unique():\n",
    "    best_of_this_class = median_aggregates[median_aggregates[\"Model Class\"]==m].set_index([\"Classifier\", \"Model\", \"Space Type\"])\n",
    "    best_of_each_class.append(best_of_this_class.iloc[[argbest_fn(best_of_this_class[\"ROC AUC\"])],:])\n",
    "\n",
    "best_of_each_class_df = pd.concat(best_of_each_class)\n",
    "\n",
    "print(\n",
    "    table_results.groupby(\n",
    "        [\"Classifier\",\"Model\",\"Space Type\",]\n",
    "    )[table_cols].agg(\n",
    "        lambda x: f\"{np.quantile(x, q=0.50):.3f} ({np.quantile(x, q=0.75)-np.quantile(x, q=0.25):.3f})\"\n",
    "    ).sort_index(inplace=False).loc[best_of_each_class_df.index].style.to_latex(\n",
    "        hrules=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clf = test_results[\"Classifier\"].unique()\n",
    "n_clf = len(unique_clf)\n",
    "\n",
    "fig, axes = plt.subplots(ncols = n_clf, sharex=True, sharey=True)\n",
    "\n",
    "for axi, clf in enumerate(unique_clf):\n",
    "    plot_results = test_results[test_results[\"Classifier\"]==clf]\n",
    "    plot_results = plot_results[\n",
    "        np.logical_or(\n",
    "            plot_results[\"Model Class\"].str.startswith(\"PAAE (KEGG)\"),\n",
    "            plot_results[\"Model Class\"].str.startswith(\"AE\"),\n",
    "            plot_results[\"Model Class\"].str.startswith(\"VAE\"),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    p = sns.scatterplot(\n",
    "        data=plot_results,\n",
    "        x=\"Model Size\",\n",
    "        y=\"ROC AUC\",\n",
    "        hue=\"Model Class\",\n",
    "        style=\"Space Type\",\n",
    "        ax=axes[axi],\n",
    "        legend=(axi==n_clf-1),\n",
    "        )\n",
    "    p.set_xscale(\"log\")\n",
    "    if (axi==n_clf-1): sns.move_legend(p, \"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    axes[axi].set_title(clf)\n",
    "    \n",
    "    for fmt in SAVING_FORMATS: plt.savefig(os.path.join(images_folder,fmt,f\"clfbrca-ext-paramplot.{fmt}\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do plot with number of layers\n",
    "unique_clf = test_results[\"Classifier\"].unique()\n",
    "unique_clf = test_results[\"Classifier\"].unique()\n",
    "n_clf = len(unique_clf)\n",
    "\n",
    "fig, axes = plt.subplots(ncols = n_clf, sharex=True, sharey=True)\n",
    "\n",
    "for axi, clf in enumerate(unique_clf):\n",
    "    plot_results = test_results[test_results[\"Classifier\"]==clf]\n",
    "    plot_results[\"Nonlinear Layers\"] = plot_results[\"Model\"].str.count(\",\")\n",
    "    plot_results = plot_results[\n",
    "        np.logical_or(\n",
    "            plot_results[\"Model Class\"].str.startswith(\"PAAE (KEGG)\"),\n",
    "            plot_results[\"Model Class\"].str.startswith(\"AE\"),\n",
    "            plot_results[\"Model Class\"].str.startswith(\"VAE\"),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    p = sns.scatterplot(\n",
    "        data=plot_results,\n",
    "        x=\"Nonlinear Layers\",\n",
    "        y=\"ROC AUC\",\n",
    "        hue=\"Model Class\",\n",
    "        style=\"Space Type\",\n",
    "        ax=axes[axi],\n",
    "        legend=(axi==n_clf-1),\n",
    "        )\n",
    "    if (axi==n_clf-1): sns.move_legend(p, \"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    axes[axi].set_title(clf)\n",
    "    \n",
    "    for fmt in SAVING_FORMATS: plt.savefig(os.path.join(images_folder,fmt,f\"clfbrca-ext-layerplot.{fmt}\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_normtype = \"log2p1e-3_fpkm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_models = [\n",
    "    (\"AE-[128, 64]\",\"LR\",\"$z$\"),\n",
    "    (\"PAAE-[32]-[64] (KEGG)\",\"LR\",\"$z$\"),\n",
    "    (\"PAAE-[32]-[64] (KEGG)\",\"LR\",\"$a$\"),\n",
    "    (\"PAAE-[32]-[64] (Hallmark Genes)\",\"LR\",\"$z$\"),\n",
    "    (\"PAAE-[32]-[64] (Hallmark Genes)\",\"LR\",\"$a$\"),\n",
    "    (\"PAVAE-smooth-β1-[]-[128, 64] (KEGG)\",\"SVM\",\"$μ$\"),\n",
    "    (\"PAVAE-smooth-β1-[]-[128, 64] (KEGG)\",\"SVM\",\"$a$\"),\n",
    "    (\"PAVAE-step-β1-[]-[128, 64] (Hallmark Genes)\",\"SVM\",\"$μ$\"),\n",
    "    (\"PAVAE-step-β1-[]-[128, 64] (Hallmark Genes)\",\"SVM\",\"$a$\"),\n",
    "    (\"VAE-step-β1-[128, 64]\",\"LR\",\"$μ$\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = set(test_results[\"Model\"].unique())\n",
    "\n",
    "DEFAULT_MODEL_REPORTING_DEFINITIONS = [\n",
    "    (False, \"paper\", set(paper_models)\n",
    "    ),\n",
    "]\n",
    "\n",
    "DEFAULT_HUE_FN = lambda this_df: this_df[\"Model\"].str.split(\"-\").str[0]\n",
    "DEFAULT_HUE_ORDER = sorted(test_results[\"Model\"].str.split(\"-\").str[0].unique())\n",
    "\n",
    "def plot_fn(plot_var,\n",
    "        clusion_names_to_show = None,\n",
    "        figsize_to_show = \"wideadj\",\n",
    "        model_reporting_definitions = DEFAULT_MODEL_REPORTING_DEFINITIONS,\n",
    "        hue_fn = DEFAULT_HUE_FN,\n",
    "        hue_order = DEFAULT_HUE_ORDER,\n",
    "        plot_legend=False,\n",
    "        sort_by_var=False,\n",
    "        sort_descending=True,\n",
    "        savefigs=True,\n",
    "        xlim=None,\n",
    "        plot_kwargs={}):\n",
    "\n",
    "    for is_exclusion, set_name, model_set in model_reporting_definitions:\n",
    "\n",
    "        is_included = test_results[[\"Model\",\"Classifier\",\"Space Type\"]].apply(lambda x: tuple(x) in model_set, axis=1)\n",
    "        if is_exclusion: is_included = np.logical_not(is_included)\n",
    "        \n",
    "        this_data = test_results[is_included].replace([np.inf,-np.inf],np.nan).dropna(subset=[plot_var]).copy()\n",
    "        plot_col = \"(Model, Classifier, Space)\"\n",
    "        this_data[plot_col] = test_results[[\"Model\",\"Classifier\",\"Space Type\"]].apply(lambda x: str(tuple(x)), axis=1)\n",
    "        \n",
    "        this_models = this_data[plot_col].unique()\n",
    "        if sort_by_var:\n",
    "            model_name_and_median = list(zip(this_models, [this_data.loc[this_data[plot_col]==m,plot_var].median() for m in this_models]))\n",
    "            model_name_and_median = sorted(\n",
    "                model_name_and_median,\n",
    "                key=lambda x: tuple(reversed(x)),\n",
    "                reverse=sort_descending,\n",
    "            )\n",
    "            this_models = [model for model, _ in model_name_and_median]\n",
    "\n",
    "        if this_data.shape[0]<=0:\n",
    "            continue\n",
    "        this_hue = this_data[\"Model\"].str.split(\"-\").str[0]\n",
    "        num_models = len(this_data[plot_col].unique())\n",
    "        for figsizename, figsize in [\n",
    "            (\"default\",None),\n",
    "            (\"thinadj\",(3,6/20*num_models)),\n",
    "            (\"wideadj\",(7,6/20*num_models)),\n",
    "        ]:\n",
    "            plt.figure(figsize=figsize)\n",
    "            \n",
    "            sns.boxplot(data = this_data,\n",
    "                x = plot_var, y = plot_col,\n",
    "                hue=this_hue, dodge=False,\n",
    "                hue_order=hue_order,\n",
    "                order=this_models,\n",
    "                **plot_kwargs)\n",
    "            plt.xlim(xlim)\n",
    "            if not plot_legend:\n",
    "                plt.legend([],[], frameon=False)\n",
    "            if savefigs:\n",
    "                for fmt in SAVING_FORMATS: plt.savefig(os.path.join(images_folder,fmt,f\"clfbrca-ext-boxplot-{plot_var.lower().replace('-','_').replace(' ','_')}-{set_name}-{figsizename}.{fmt}\"), bbox_inches=\"tight\")\n",
    "            if ((clusion_names_to_show is None or set_name in clusion_names_to_show\n",
    "                ) and (\n",
    "                figsize_to_show==figsizename\n",
    "                )):\n",
    "                print(set_name, figsizename)\n",
    "                plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(\"ROC AUC\", sort_by_var=True, sort_descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(\"Accuracy\", sort_by_var=True, sort_descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(\"F1\", sort_by_var=True, sort_descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(\"Train MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(\"Test MSE\", xlim=[0,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(\"Fit Time\", savefigs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_table(\n",
    "        this_data,\n",
    "        plot_var,\n",
    "        col_name=\"ROC AUC\",\n",
    "        table_is_sorted=True,\n",
    "        bigger_is_better=True,\n",
    "        cmp_col_fn = lambda df: df[\"Model\"]\n",
    "        ):\n",
    "    cmp_col = cmp_col_fn(this_data)\n",
    "    comparison_models = cmp_col.unique()\n",
    "    num_models = len(comparison_models)\n",
    "    model_name_and_median = list(zip(comparison_models, [this_data.loc[cmp_col==m,col_name].median() for m in comparison_models], [this_data.loc[cmp_col==m,col_name].count() for m in comparison_models]))\n",
    "    if table_is_sorted:\n",
    "        model_name_and_median = sorted(\n",
    "            model_name_and_median,\n",
    "            key=lambda x: x[1],\n",
    "            reverse=bigger_is_better\n",
    "        )\n",
    "\n",
    "    model_comparison_df_p_value_data = {\n",
    "        n: [] for (n, *_) in model_name_and_median\n",
    "    }\n",
    "    model_comparison_df_bigger_data = {\n",
    "        n: [] for (n, *_) in model_name_and_median\n",
    "    }\n",
    "    model_comparison_df_plot_data = {\n",
    "        n: [] for (n, *_) in model_name_and_median\n",
    "    }\n",
    "    model_comparison_df_index = []\n",
    "\n",
    "    for i in range(len(model_name_and_median)):\n",
    "        ni = model_name_and_median[i][0]\n",
    "        dfi = this_data[cmp_col==ni]\n",
    "        model_comparison_df_index.append(ni)\n",
    "        for j in range(len(model_name_and_median)):\n",
    "            nj = model_name_and_median[j][0]\n",
    "            dfj = this_data[cmp_col==nj]\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    wcx = sp.stats.wilcoxon(dfi[plot_var], dfj[plot_var], nan_policy=\"omit\")[1]\n",
    "            except:\n",
    "                wcx = np.nan\n",
    "            i_bigger_than_j = dfi[plot_var].median()>dfj[plot_var].median()\n",
    "            cmp_symbol = \"-\" if np.isnan(wcx)  else (('$>$' if i_bigger_than_j else '$<$') if wcx<0.05 else \"$\\\\approx$\")\n",
    "            p_value_str = \"-\" if np.isnan(wcx) else (f\"$p={wcx:.3f}$\" if wcx>=1e-3 else \"$p<10^{-3}$\")\n",
    "\n",
    "            model_comparison_df_p_value_data[nj].append(wcx)\n",
    "            model_comparison_df_bigger_data[nj].append(i_bigger_than_j)\n",
    "            model_comparison_df_plot_data[nj].append(f\"{cmp_symbol} ({p_value_str})\")\n",
    "\n",
    "    df_model_comparison = pd.DataFrame(model_comparison_df_p_value_data, index=model_comparison_df_index)\n",
    "    df_model_comparison_plot = pd.DataFrame(model_comparison_df_plot_data, index=model_comparison_df_index)\n",
    "    return model_name_and_median, df_model_comparison, df_model_comparison_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_col_fn = lambda df: df[[\"Model\",\"Classifier\",\"Space Type\"]].apply(lambda x: tuple(x), axis=1)\n",
    "paper_test_results = test_results[cmp_col_fn(test_results).isin(set(paper_models))].reset_index()\n",
    "\n",
    "model_name_and_median, auc_cmp_df, auc_cmp_plt_df = get_comparison_table(paper_test_results, \"ROC AUC\", cmp_col_fn=cmp_col_fn)\n",
    "print(model_name_and_median)\n",
    "auc_cmp_plt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_cmp_plt_df.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_col_fn = lambda df: df[[\"Model\",\"Classifier\",\"Space Type\"]].apply(lambda x: tuple(x), axis=1)\n",
    "paper_test_results = test_results[cmp_col_fn(test_results).isin(set(paper_models))].reset_index()\n",
    "\n",
    "for metric, bigger_is_better in [\n",
    "    (\"Model Size\", False),\n",
    "    (\"Test MSE\", False),\n",
    "    (\"Accuracy\", True),\n",
    "    (\"Precision\", True),\n",
    "    (\"Recall\", True),\n",
    "    (\"F1\", True),\n",
    "    (\"ROC AUC\", True),\n",
    "]:\n",
    "    model_name_and_median, auc_cmp_df, auc_cmp_plt_df = get_comparison_table(paper_test_results, metric, col_name=metric, cmp_col_fn=cmp_col_fn, bigger_is_better=bigger_is_better)\n",
    "    print(metric, (*model_name_and_median[0], *(auc_cmp_df.iloc[0,0],)), end=\"\")\n",
    "    for i in range(1, len(model_name_and_median)):\n",
    "        if auc_cmp_df.iloc[0,i] > 0.05:\n",
    "            print(\"\", (*model_name_and_median[i],*(auc_cmp_df.iloc[0,i],)), end=\"\")\n",
    "        else:\n",
    "            break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_and_median, auc_cmp_df, auc_cmp_plt_df = get_comparison_table(test_results, \"ROC AUC\")\n",
    "print(model_name_and_median)\n",
    "auc_cmp_plt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_cmp_plt_df.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aa1c49e068f69db5bcdfd06143621343d5e21d0aa95c73e4e0dd023b1fedc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
