{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots are quite big, and they weren't used for anything, but feel free to activate the plot generation if you want.\n",
    "make_plots = False\n",
    "\n",
    "SAVING_FORMATS = [\"pdf\", \"svg\", \"png\"]\n",
    "results_folder = \"results\"\n",
    "images_folder = \"images\"\n",
    "os.makedirs(images_folder, exist_ok=True)\n",
    "for fmt in SAVING_FORMATS: os.makedirs(os.path.join(images_folder,fmt), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVERTED_METRICS = [\"AE_Loss\", \"AE_MSE\", \"AE_R2\"]\n",
    "\n",
    "results_to_gather = [ # Had to run each model separately due to memory constraints\n",
    "    \"clf_valid_results_202302241145.csv\",\n",
    "    \"clf_valid_results_202302241540.csv\",\n",
    "    \"clf_valid_results_202302251829.csv\",\n",
    "    \"clf_valid_results_202302271642.csv\",\n",
    "]\n",
    "\n",
    "test_results = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(os.path.join(results_folder,f), index_col=0) for f in results_to_gather\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").reset_index()\n",
    "\n",
    "for m in INVERTED_METRICS:\n",
    "    test_results[f\"Test {m}\"] = -test_results[f\"Test {m}\"]\n",
    "\n",
    "test_results = test_results.rename(\n",
    "    columns={\n",
    "        **{old:old.replace(\"Test \", \"\") for old in test_results.columns if old.startswith(\"Test \")},\n",
    "        **{old:old.replace(\"Test AE_\", \"AE \") for old in test_results.columns if old.startswith(\"Test AE_\")},\n",
    "    }\n",
    ").rename(columns={\"AUC\":\"ROC AUC\"})\n",
    "\n",
    "test_results[\"Model\"] = test_results[\"Model\"].str.replace(\"-SVC\", \"-SVM\").str.replace(\"-LogisticRegression\", \"-LR\").str.replace(\"-RandomForestClassifier\", \"-RF\")\n",
    "\n",
    "test_results[\"AE Type\"] = test_results[\"Model\"].str.split(\"-\").str[0] + \" \" + test_results[\"Model\"].str.extract(\"(\\\\(.*\\\\))\").replace(np.nan,\"\").squeeze()\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_per_type = {}\n",
    "for typ in test_results[\"AE Type\"].unique():\n",
    "    best = test_results.loc[test_results[\"AE Type\"]==typ,\"Model\"].unique()[0]\n",
    "    best_auc = test_results.loc[test_results[\"Model\"]==best,\"ROC AUC\"].mean()\n",
    "    for this in test_results.loc[test_results[\"AE Type\"]==typ,\"Model\"].unique():\n",
    "        this_auc = test_results.loc[test_results[\"Model\"]==this,\"ROC AUC\"].mean()\n",
    "        if this_auc > best_auc:\n",
    "            best = this\n",
    "            best_auc = this_auc\n",
    "    best_models_per_type[typ] = (best,best_auc)\n",
    "with open(os.path.join(results_folder,\"best.json\"), \"w\") as best_f:\n",
    "    json.dump(best_models_per_type, best_f)\n",
    "best_models_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_model_and_feat(k):\n",
    "    if isinstance(k,(pd.Index,)):\n",
    "        idx:pd.Index = k\n",
    "        return [sep_model_and_feat(i) for i in idx.values]\n",
    "    feature = k.split(\"-\")[0] + \" \" + \" \".join(k.split(\"-\")[1].split(\" \")[1:])\n",
    "    model = k.split(\"-\")[1].split(\" \")[0]\n",
    "    return model, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_cols = [\"AE MSE\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\"]\n",
    "table_results = test_results.copy()\n",
    "table_results[\"Model\"] = table_results[\"Model\"].str.replace(\"β\",\"$\\\\beta$\")\n",
    "print(\n",
    "    table_results[table_results[\"Input Normalization\"]==\"log2p1e-3_fpkm\"].groupby(\n",
    "        \"Model\"\n",
    "    )[table_cols].agg(\n",
    "        lambda x: f\"{np.quantile(x, q=0.50):.3f} ({np.quantile(x, q=0.75)-np.quantile(x, q=0.25):.3f})\"\n",
    "    ).sort_index(key=sep_model_and_feat,inplace=False).style.to_latex(\n",
    "        hrules=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_cols = [\"AE MSE\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\"]\n",
    "table_results = test_results.copy()\n",
    "table_results[\"Model\"] = table_results[\"Model\"].str.replace(\"β\",\"$\\\\beta$\")\n",
    "print(\n",
    "    table_results[table_results[\"Input Normalization\"]==\"log2p1e-3_fpkm\"].groupby(\n",
    "        \"Model\"\n",
    "    )[table_cols].agg(\n",
    "        lambda x: f\"${np.mean(x):.3f}\\\\pm {np.std(x):.3f}$\"\n",
    "    ).sort_index(key=sep_model_and_feat,inplace=False).style.to_latex(\n",
    "        hrules=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"Model\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_normtype = \"log2p1e-3_fpkm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = set(test_results[\"Model\"].unique())\n",
    "\n",
    "DEFAULT_MODEL_REPORTING_DEFINITIONS = [\n",
    "    (False, \"all\", all_models),\n",
    "    (False, \"svm\", {m for m in all_models if \"-SVM\" in m}),\n",
    "    (False, \"rf\", {m for m in all_models if \"-RF\" in m}),\n",
    "    (False, \"lr\", {m for m in all_models if \"-LR\" in m}),\n",
    "]\n",
    "\n",
    "DEFAULT_HUE_FN = lambda this_df: this_df[\"Model\"].str.split(\"-\").str[0]\n",
    "DEFAULT_HUE_ORDER = sorted(test_results[\"Model\"].str.split(\"-\").str[0].unique())\n",
    "\n",
    "def plot_fn(plot_var,\n",
    "        normtypes_to_show = None,\n",
    "        clusion_names_to_show = None,\n",
    "        figsize_to_show = \"wideadj\",\n",
    "        model_reporting_definitions = DEFAULT_MODEL_REPORTING_DEFINITIONS,\n",
    "        hue_fn = DEFAULT_HUE_FN,\n",
    "        hue_order = DEFAULT_HUE_ORDER,\n",
    "        plot_legend=False,\n",
    "        sort_by_var=False,\n",
    "        sort_descending=True,\n",
    "        savefigs=True,\n",
    "        xlim=None,\n",
    "        plot_kwargs={}):\n",
    "    all_models = set(test_results[\"Model\"].unique())\n",
    "\n",
    "    for normtype in sorted(test_results[\"Input Normalization\"].unique(), key=lambda x: x[-1]+x):\n",
    "        for is_exclusion, set_name, model_set in model_reporting_definitions:\n",
    "\n",
    "            is_included = test_results[\"Model\"].isin(model_set)\n",
    "            if is_exclusion: is_included = np.logical_not(is_included)\n",
    "\n",
    "            is_normtype = test_results[\"Input Normalization\"]==normtype\n",
    "            \n",
    "            this_data = test_results[np.logical_and(test_results[\"External Split\"]!=0,np.logical_and(is_normtype,is_included))].replace([np.inf,-np.inf],np.nan).dropna(subset=[plot_var]).copy()\n",
    "            this_models = this_data[\"Model\"].unique()\n",
    "            if sort_by_var:\n",
    "                model_name_and_median = list(zip(this_models, [this_data.loc[this_data[\"Model\"]==m,plot_var].median() for m in this_models]))\n",
    "                model_name_and_median = sorted(\n",
    "                    model_name_and_median,\n",
    "                    key=lambda x: tuple(reversed(x)),\n",
    "                    reverse=sort_descending,\n",
    "                )\n",
    "                this_models = [model for model, _ in model_name_and_median]\n",
    "            \n",
    "            if isinstance(model_set, dict) and not is_exclusion:\n",
    "                this_data.loc[:,\"Model\"] = this_data[\"Model\"].map(model_set)\n",
    "            if this_data.shape[0]<=0:\n",
    "                continue\n",
    "            this_hue = this_data[\"Model\"].str.split(\"-\").str[0]\n",
    "            num_models = len(this_data[\"Model\"].unique())\n",
    "            for figsizename, figsize in [\n",
    "                (\"default\",None),\n",
    "                (\"thinadj\",(3,6/20*num_models)),\n",
    "                (\"wideadj\",(7,6/20*num_models)),\n",
    "            ]:\n",
    "                plt.figure(figsize=figsize)\n",
    "                \n",
    "                sns.boxplot(data = this_data,\n",
    "                    x = plot_var, y = \"Model\",\n",
    "                    hue=this_hue, dodge=False,\n",
    "                    hue_order=hue_order,\n",
    "                    order=this_models,\n",
    "                    **plot_kwargs)\n",
    "                plt.xlim(xlim)\n",
    "                if not plot_legend:\n",
    "                    plt.legend([],[], frameon=False)\n",
    "                if savefigs:\n",
    "                    for fmt in SAVING_FORMATS: plt.savefig(os.path.join(images_folder,fmt,f\"clfbrca-boxplot-{plot_var.lower().replace('-','_').replace(' ','_')}-{normtype}-{set_name}-{figsizename}.{fmt}\"), bbox_inches=\"tight\")\n",
    "                if ((clusion_names_to_show is None or set_name in clusion_names_to_show\n",
    "                    ) and (\n",
    "                    normtypes_to_show is None or normtype in normtypes_to_show\n",
    "                    ) and (\n",
    "                    figsize_to_show==figsizename\n",
    "                    )):\n",
    "                    print(normtype, set_name, figsizename)\n",
    "                    plt.show()\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots: plot_fn(\"ROC AUC\", sort_by_var=True, sort_descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots: plot_fn(\"Accuracy\", sort_by_var=True, sort_descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots: plot_fn(\"F1\", sort_by_var=True, sort_descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots: plot_fn(\"AE MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_plots: plot_fn(\"Fit Time\", savefigs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_table(this_data, plot_var, col_name=\"ROC AUC\", table_is_sorted=True, bigger_is_better=True):\n",
    "    comparison_models = this_data[\"Model\"].unique()\n",
    "    num_models = len(comparison_models)\n",
    "    model_name_and_median = list(zip(comparison_models, [this_data.loc[this_data[\"Model\"]==m,col_name].median() for m in comparison_models], [this_data.loc[this_data[\"Model\"]==m,col_name].count() for m in comparison_models]))\n",
    "    if table_is_sorted:\n",
    "        model_name_and_median = sorted(\n",
    "            model_name_and_median,\n",
    "            key=lambda x: x[1],\n",
    "            reverse=bigger_is_better\n",
    "        )\n",
    "\n",
    "    model_comparison_df_p_value_data = {\n",
    "        n: [] for (n, *_) in model_name_and_median\n",
    "    }\n",
    "    model_comparison_df_bigger_data = {\n",
    "        n: [] for (n, *_) in model_name_and_median\n",
    "    }\n",
    "    model_comparison_df_plot_data = {\n",
    "        n: [] for (n, *_) in model_name_and_median\n",
    "    }\n",
    "    model_comparison_df_index = []\n",
    "\n",
    "    for i in range(len(model_name_and_median)):\n",
    "        ni = model_name_and_median[i][0]\n",
    "        dfi = this_data[this_data[\"Model\"]==ni]\n",
    "        model_comparison_df_index.append(ni)\n",
    "        for j in range(len(model_name_and_median)):\n",
    "            nj = model_name_and_median[j][0]\n",
    "            dfj = this_data[this_data[\"Model\"]==nj]\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    wcx = sp.stats.wilcoxon(dfi[plot_var], dfj[plot_var])[1]\n",
    "            except:\n",
    "                wcx = np.nan\n",
    "            i_bigger_than_j = dfi[plot_var].median()>dfj[plot_var].median()\n",
    "            cmp_symbol = \"-\" if np.isnan(wcx)  else (('$>$' if i_bigger_than_j else '$<$') if wcx<0.05 else \"$\\\\approx$\")\n",
    "            p_value_str = \"-\" if np.isnan(wcx) else (f\"$p={wcx:.3f}$\" if wcx>=1e-3 else \"$p<10^{-3}$\")\n",
    "\n",
    "            model_comparison_df_p_value_data[nj].append(wcx)\n",
    "            model_comparison_df_bigger_data[nj].append(i_bigger_than_j)\n",
    "            model_comparison_df_plot_data[nj].append(f\"{cmp_symbol} ({p_value_str})\")\n",
    "\n",
    "    df_model_comparison = pd.DataFrame(model_comparison_df_p_value_data, index=model_comparison_df_index)\n",
    "    df_model_comparison_plot = pd.DataFrame(model_comparison_df_plot_data, index=model_comparison_df_index)\n",
    "    return model_name_and_median, df_model_comparison, df_model_comparison_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_and_median, auc_cmp_df, auc_cmp_plt_df = get_comparison_table(test_results, \"ROC AUC\")\n",
    "print(model_name_and_median)\n",
    "auc_cmp_plt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_cmp_plt_df.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aa1c49e068f69db5bcdfd06143621343d5e21d0aa95c73e4e0dd023b1fedc27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
